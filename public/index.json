[{"authors":["admin"],"categories":null,"content":"During my time as a Research Analyst, I utilized R, Python, and Excel to manage and analyze complex datasets, ensuring the accuracy and depth of research findings. My role involved improving evaluation instruments and applying advanced statistical methods to derive meaningful insights. As an Assessment Assistant, I diagnosed and resolved hardware and software issues related to data science tools, developed resources to improve students\u0026rsquo; troubleshooting skills, and collaborated with faculty to address technical challenges. Previously, as a Digital Creator, I successfully managed a social media account with a significant following, conducting in-depth data analysis that drove strategic decision-making and resulted in substantial revenue growth. I also performed trend analysis and implemented data cleaning and pre-processing techniques using Excel and SQL on a self-hosted MariaDB server. Resume/CV is available upon request.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://localhost:4321/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"During my time as a Research Analyst, I utilized R, Python, and Excel to manage and analyze complex datasets, ensuring the accuracy and depth of research findings. My role involved improving evaluation instruments and applying advanced statistical methods to derive meaningful insights. As an Assessment Assistant, I diagnosed and resolved hardware and software issues related to data science tools, developed resources to improve students\u0026rsquo; troubleshooting skills, and collaborated with faculty to address technical challenges.","tags":null,"title":"Manuel Cortez","type":"authors"},{"authors":null,"categories":null,"content":" ","date":1708819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708819200,"objectID":"28807482ca68cc65ef826c3b6d6e358a","permalink":"http://localhost:4321/post/ethics_data_science/","publishdate":"2024-02-25T00:00:00Z","relpermalink":"/post/ethics_data_science/","section":"post","summary":"How Deepfake Technology Harms Society","tags":null,"title":"Ethics in Data Science","type":"post"},{"authors":null,"categories":null,"content":"In this project, I built and evaluated a Hidden Markov Model (HMM) with a Viterbi algorithm for part-of-speech tagging using the Brown corpus. After pre-processing the data and splitting it into training and test sets, I trained five different HMM models with various smoothing techniques, such as Lidstone, Maximum Likelihood Estimation (MLE), and Expected Likelihood Estimation (ELE). I then evaluated each model by predicting tags on the test dataset and compared their performance using metrics like precision, recall, and F1-score. I analyzed the results to identify the best-performing model, taking into account the effects of different smoothing techniques and comparing them to a pure HMM. Finally, I selected the best model and serialized it with the dill library.\nSee github repository for full project. Available upon request.\n","date":1708819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708819200,"objectID":"4cb1cf7445e7a9692951dd98de4f59c9","permalink":"http://localhost:4321/project/hmm_viterbi_nlp/","publishdate":"2024-02-25T00:00:00Z","relpermalink":"/project/hmm_viterbi_nlp/","section":"project","summary":"Building and Evaluating a Hidden Markov Model and a Viterbi Algorithm in NLP","tags":null,"title":"HMM and Viterbi in NLP","type":"project"},{"authors":null,"categories":null,"content":"See github repository for full project. Available upon request.\n","date":1707955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707955200,"objectID":"b39dfb7f9d7906a1e59a6d811a7673e9","permalink":"http://localhost:4321/project/web_scraping_in_r/","publishdate":"2024-02-15T00:00:00Z","relpermalink":"/project/web_scraping_in_r/","section":"project","summary":"R Script Used to Extract Data From Websites","tags":null,"title":"Web Scraping Script","type":"project"},{"authors":null,"categories":null,"content":"In this project, I focused on predicting whether a mushroom is poisonous or not using K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA). The mushroom dataset was downloaded from the UCI library. Upon discovering missing data, I split the dataset into two parts: one with missing data (used as the test set) and one without (used as the training set). I then one-hot encoded the feature data and label encoded the response variable.\nAfter ensuring that the feature-training and feature-testing data were consistent, I trained a KNN model on the training data and used it to impute the missing values. I verified that the imputed values matched the original number of missing entries, then reintegrated them into the dataset. Next, I split the data into features and response variables and used train_test_split() to further divide it into training and testing sets.\nI trained a Random Forest classifier and a Logistic Regression model on the data, calculating their accuracy, precision, and recall. To optimize the models, I applied PCA to reduce the dimensionality of the feature data, retaining 95% of the variance. I re-ran the models and recalculated their performance metrics. Finally, I tabulated the results, including accuracy, precision, recall, and computation time, and provided a detailed explanation as to which model was preferable.\nSee github repository for full project. Available upon request.\n","date":1702166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702166400,"objectID":"0c371d0ad38672212dd937ac7bdb2179","permalink":"http://localhost:4321/project/mushroom_classifier/","publishdate":"2023-12-10T00:00:00Z","relpermalink":"/project/mushroom_classifier/","section":"project","summary":"Analysis of Mushroom Toxicity","tags":null,"title":"Mushroom Classifier","type":"project"},{"authors":null,"categories":null,"content":"In this project, I applied database design and query development skills to create and manage a banking database for a University\u0026rsquo;s Bank Holdings. The first part involved designing the database schema, where I defined primary and foreign keys, implemented constraints, and ensured data integrity through a comprehensive Data Definition Language (DDL). After populating the database with provided data, I wrote and tested a series of complex queries utilizing JOINs, SET operators, and subqueries. Additionally, I developed advanced queries for a university\u0026rsquo;s student database, further applying SQL concepts that meet specific data retrieval and analysis needs.\nSee github repository for full project. Available upon request.\n","date":1701129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701129600,"objectID":"7671cf24368534625e5f8f1e97336e4e","permalink":"http://localhost:4321/project/banking_university_database/","publishdate":"2023-11-28T00:00:00Z","relpermalink":"/project/banking_university_database/","section":"project","summary":"Creation and Queries to a University and Banking Database","tags":null,"title":"University Banking Database","type":"project"},{"authors":null,"categories":null,"content":"The project scenario consists of being hired as a contractor to help a university migrate an existing student system to a new platform using C++ language. My primary responsibility was to develop a program that included two key classes: Student and Roster. This program was designed to manage the current roster of students enrolled in a course. It involved reading student data, including ID, name, email, age, course completion times, and degree program, and using function calls to manipulate this data. As the data was parsed, the program created student objects and stored the entire student list in an array called classRosterArray. The program provided specific outputs directly to the console, ensuring accurate and efficient data management throughout the migration process.\nSee github repository for full project. Available upon request.\n","date":1659484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659484800,"objectID":"5a75906aee40a4c51f27691d3b8a1029","permalink":"http://localhost:4321/project/class_roster_project/","publishdate":"2022-08-03T00:00:00Z","relpermalink":"/project/class_roster_project/","section":"project","summary":"The Final Project to the Scripting and Programming Applications Course","tags":null,"title":"Class Roster Project","type":"project"},{"authors":null,"categories":null,"content":"The project scenario consists of being hired as a junior data analyst on the marketing analyst team at Cyclistic, a bike-share company in Chicago. The main objective was to understand the differences in usage patterns between annual members and casual riders. By analyzing data on ride frequency, duration, and popular routes, I identified key trends that explained why casual riders might consider upgrading to annual memberships. The insights I gathered were instrumental in designing targeted marketing strategies aimed at converting casual riders into loyal annual members. My findings were presented to Cyclistic executives, backed by compelling data visualizations and actionable recommendations. The main tool that was used in this analysis was R.\nSee github repository for full project. Available upon request.\n","date":1657411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657411200,"objectID":"51727b135e50e7eb616b53ac75962104","permalink":"http://localhost:4321/project/google_data_analytics_capstone/","publishdate":"2022-07-10T00:00:00Z","relpermalink":"/project/google_data_analytics_capstone/","section":"project","summary":"The Capstone Project for the Google Data Analytics Specialization","tags":null,"title":"Analysis Of 2019 Cyclistic Bike Share Data","type":"project"},{"authors":null,"categories":null,"content":"In this project, a ZIP file containing newspaper images were processed. The goal was to develop a Python program capable of searching these images for specific keywords and detecting faces associated with those keywords. To achieve this, I applied several Python libraries, including Pillow for image manipulation, OpenCV for face detection, and Pytesseract for optical character recognition (OCR). The project involved creating a contact sheet of all faces found on newspaper pages that mentioned the search term. This comprehensive project tested my ability to learn new tools, integrate multiple libraries, and solve complex problems in image processing.\nSee github repository for full project. Available upon request.\n","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"bed7a28a9302be19d22a0bcbf3a301e2","permalink":"http://localhost:4321/project/python_3_programming_capstone/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/project/python_3_programming_capstone/","section":"project","summary":"The Final Project for the Python 3 Programming course from the University of Michigan","tags":null,"title":"Python 3 Programming Project","type":"project"}]